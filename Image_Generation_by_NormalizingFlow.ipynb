{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g13DOStGtOOp"
      ],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Import required package"
      ],
      "metadata": {
        "id": "g13DOStGtOOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEREixiCi4O9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision as tv\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Move model on GPU if available\n",
        "enable_cuda = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Define 2D Gaussian base distribution & target"
      ],
      "metadata": {
        "id": "GTAJrG_XtTDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_log_det_like_z(z):\n",
        "    return torch.zeros(z.shape[0], dtype=z.dtype, device=z.device)\n",
        "\n",
        "# Base Distribution\n",
        "class DiagGaussian(nn.Module):\n",
        "    # Multivariate Gaussian distribution with diagonal covariance matrix\n",
        "\n",
        "    def __init__(self, shape, trainable=True):\n",
        "        super().__init__()\n",
        "        if isinstance(shape, int):\n",
        "            shape = (shape,)\n",
        "        if isinstance(shape, list):\n",
        "            shape = tuple(shape)\n",
        "        self.shape = shape\n",
        "        self.n_dim = len(shape)\n",
        "        self.d = np.prod(shape)\n",
        "        if trainable:\n",
        "            self.loc = nn.Parameter(torch.zeros(1, *self.shape))\n",
        "            self.log_scale = nn.Parameter(torch.zeros(1, *self.shape))\n",
        "        else:\n",
        "            self.register_buffer(\"loc\", torch.zeros(1, *self.shape))\n",
        "            self.register_buffer(\"log_scale\", torch.zeros(1, *self.shape))\n",
        "        self.temperature = None  # Temperature parameter for annealed sampling\n",
        "\n",
        "    def forward(self, num_samples=1, context=None):\n",
        "        eps = torch.randn(\n",
        "            (num_samples,) + self.shape, dtype=self.loc.dtype, device=self.loc.device\n",
        "        )\n",
        "        if self.temperature is None:\n",
        "            log_scale = self.log_scale\n",
        "        else:\n",
        "            log_scale = self.log_scale + np.log(self.temperature)\n",
        "        z = self.loc + torch.exp(log_scale) * eps\n",
        "        log_p = -0.5 * self.d * np.log(2 * np.pi) - torch.sum(\n",
        "            log_scale + 0.5 * torch.pow(eps, 2), list(range(1, self.n_dim + 1))\n",
        "        )\n",
        "        return z, log_p\n",
        "\n",
        "    def log_prob(self, z, context=None):\n",
        "        if self.temperature is None:\n",
        "            log_scale = self.log_scale\n",
        "        else:\n",
        "            log_scale = self.log_scale + np.log(self.temperature)\n",
        "        log_p = -0.5 * self.d * np.log(2 * np.pi) - torch.sum(\n",
        "            log_scale + 0.5 * torch.pow((z - self.loc) / torch.exp(log_scale), 2),\n",
        "            list(range(1, self.n_dim + 1)),\n",
        "        )\n",
        "        return log_p\n",
        "\n",
        "    def sample(self, num_samples=1, **kwargs):\n",
        "        # Samples from base distribution\n",
        "\n",
        "        z, _ = self.forward(num_samples, **kwargs)\n",
        "        return z"
      ],
      "metadata": {
        "id": "tEULi3cHkmQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CircularGaussianMixture(nn.Module):\n",
        "    # 8 gaussian distribution\n",
        "\n",
        "    def __init__(self, n_modes=8):\n",
        "        super(CircularGaussianMixture, self).__init__()\n",
        "        self.n_modes = n_modes\n",
        "        self.register_buffer(\n",
        "            \"scale\", torch.tensor(2 / 3 * np.sin(np.pi / self.n_modes)).float()\n",
        "        )\n",
        "\n",
        "    def log_prob(self, z):\n",
        "        d = torch.zeros((len(z), 0), dtype=z.dtype, device=z.device)\n",
        "        for i in range(self.n_modes):\n",
        "            d_ = (\n",
        "                (z[:, 0] - 2 * np.sin(2 * np.pi / self.n_modes * i)) ** 2\n",
        "                + (z[:, 1] - 2 * np.cos(2 * np.pi / self.n_modes * i)) ** 2\n",
        "            ) / (2 * self.scale**2)\n",
        "            d = torch.cat((d, d_[:, None]), 1)\n",
        "        log_p = -torch.log(\n",
        "            2 * np.pi * self.scale**2 * self.n_modes\n",
        "        ) + torch.logsumexp(-d, 1)\n",
        "        return log_p\n",
        "\n",
        "    def sample(self, num_samples=1):\n",
        "        eps = torch.randn(\n",
        "            (num_samples, 2), dtype=self.scale.dtype, device=self.scale.device\n",
        "        )\n",
        "        phi = (\n",
        "            2\n",
        "            * np.pi\n",
        "            / self.n_modes\n",
        "            * torch.randint(0, self.n_modes, (num_samples,), device=self.scale.device)\n",
        "        )\n",
        "        loc = torch.stack((2 * torch.sin(phi), 2 * torch.cos(phi)), 1).type(eps.dtype)\n",
        "        return eps * self.scale + loc\n"
      ],
      "metadata": {
        "id": "w5PCCYMatdzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Define RealNVP models"
      ],
      "metadata": {
        "id": "O_8q3g71zR8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flow(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, z):\n",
        "        raise NotImplementedError(\"Forward pass has not been implemented.\")\n",
        "\n",
        "    def inverse(self, z):\n",
        "        raise NotImplementedError(\"This flow has no algebraic inverse.\")"
      ],
      "metadata": {
        "id": "Hm79-23AlzYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flow layers to reshape the latent features\n",
        "\n",
        "class Split(Flow):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, z):\n",
        "        z1, z2 = z.chunk(2, dim=1)\n",
        "        log_det = 0\n",
        "        return [z1, z2], log_det\n",
        "\n",
        "    def inverse(self, z):\n",
        "        z1, z2 = z\n",
        "        z = torch.cat([z1, z2], 1)\n",
        "        log_det = 0\n",
        "        return z, log_det\n",
        "\n",
        "\n",
        "\"\"\"\n",
        " Merge - Same as Split but with forward and backward pass interchanged\n",
        "\"\"\"\n",
        "class Merge(Split):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, z):\n",
        "        return super().inverse(z)\n",
        "\n",
        "    def inverse(self, z):\n",
        "        return super().forward(z)\n"
      ],
      "metadata": {
        "id": "qfQ9zaULo2ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Permute(Flow):\n",
        "    def __init__(self, num_channels):\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        perm = torch.randperm(self.num_channels)\n",
        "        inv_perm = torch.empty_like(perm).scatter_(\n",
        "            dim=0, index=perm, src=torch.arange(self.num_channels)\n",
        "        )\n",
        "        self.register_buffer(\"perm\", perm)\n",
        "        self.register_buffer(\"inv_perm\", inv_perm)\n",
        "\n",
        "    def forward(self, z, context=None):\n",
        "        z = z[:, self.perm, ...]\n",
        "        log_det = torch.zeros(len(z), device=z.device)\n",
        "        return z, log_det\n",
        "\n",
        "    def inverse(self, z, context=None):\n",
        "        z = z[:, self.inv_perm, ...]\n",
        "        log_det = torch.zeros(len(z), device=z.device)\n",
        "        return z, log_det"
      ],
      "metadata": {
        "id": "pakdGTO3w5P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AffineCoupling(Flow):\n",
        "    def __init__(self, param_map):\n",
        "        super().__init__()\n",
        "        self.add_module(\"param_map\", param_map)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z1, z2 = z\n",
        "        param = self.param_map(z1)\n",
        "        shift = param[:, 0::2, ...]\n",
        "        scale_ = param[:, 1::2, ...]\n",
        "        z2 = z2 * torch.exp(scale_) + shift\n",
        "        log_det = torch.sum(scale_, dim=list(range(1, shift.dim())))\n",
        "\n",
        "        return [z1, z2], log_det\n",
        "\n",
        "    def inverse(self, z):\n",
        "        z1, z2 = z\n",
        "        param = self.param_map(z1)\n",
        "        shift = param[:, 0::2, ...]\n",
        "        scale_ = param[:, 1::2, ...]\n",
        "        z2 = (z2 - shift) * torch.exp(-scale_)\n",
        "        log_det = -torch.sum(scale_, dim=list(range(1, shift.dim())))\n",
        "\n",
        "        return [z1, z2], log_det\n"
      ],
      "metadata": {
        "id": "uSmhgKYcpBpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Affine Coupling layer including split and merge operation\n",
        "\"\"\"\n",
        "\n",
        "class AffineCouplingBlock(Flow):\n",
        "    def __init__(self, param_map):\n",
        "        super().__init__()\n",
        "        self.flows = nn.ModuleList([])\n",
        "        # Split layer\n",
        "        self.flows += [Split()]\n",
        "        # Affine coupling layer\n",
        "        self.flows += [AffineCoupling(param_map)]\n",
        "        # Merge layer\n",
        "        self.flows += [Merge()]\n",
        "\n",
        "    def forward(self, z):\n",
        "        log_det_tot = torch.zeros(z.shape[0], dtype=z.dtype, device=z.device)\n",
        "        for flow in self.flows:\n",
        "            z, log_det = flow(z)\n",
        "            log_det_tot += log_det\n",
        "\n",
        "        return z, log_det_tot\n",
        "\n",
        "    def inverse(self, z):\n",
        "        log_det_tot = torch.zeros(z.shape[0], dtype=z.dtype, device=z.device)\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            z, log_det = self.flows[i].inverse(z)\n",
        "            log_det_tot += log_det\n",
        "\n",
        "        return z, log_det_tot\n"
      ],
      "metadata": {
        "id": "BNkUqCewlhOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalizingFlow(nn.Module):\n",
        "\n",
        "    def __init__(self, q0, flows, p=None):\n",
        "        super().__init__()\n",
        "        self.q0 = q0 # Base distribution\n",
        "        self.flows = nn.ModuleList(flows) # list of flows\n",
        "        self.p = p # Target distribution\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        for flow in self.flows:\n",
        "            z, _ = flow(z)\n",
        "        return z\n",
        "\n",
        "    def forward_and_log_det(self, z):\n",
        "\n",
        "        log_det = torch.zeros(len(z), device=z.device)\n",
        "        for flow in self.flows:\n",
        "            z, log_d = flow(z)\n",
        "            log_det += log_d\n",
        "        return z, log_det\n",
        "\n",
        "    def inverse(self, x):\n",
        "\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            x, _ = self.flows[i].inverse(x)\n",
        "        return x\n",
        "\n",
        "    def inverse_and_log_det(self, x):\n",
        "\n",
        "        log_det = torch.zeros(len(x), device=x.device)\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            x, log_d = self.flows[i].inverse(x)\n",
        "            log_det += log_d\n",
        "        return x, log_det\n",
        "\n",
        "    def forward_kld(self, x):\n",
        "        # Estimates forward KL divergence\n",
        "\n",
        "        log_q = torch.zeros(len(x), device=x.device)\n",
        "        z = x\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            z, log_det = self.flows[i].inverse(z)\n",
        "            log_q += log_det\n",
        "        log_q += self.q0.log_prob(z)\n",
        "        return -torch.mean(log_q)\n",
        "\n",
        "    def reverse_kld(self, num_samples=1, beta=1.0, score_fn=True):\n",
        "        # Estimates reverse KL divergence\n",
        "\n",
        "        z, log_q_ = self.q0(num_samples)\n",
        "        log_q = torch.zeros_like(log_q_)\n",
        "        log_q += log_q_\n",
        "        for flow in self.flows:\n",
        "            z, log_det = flow(z)\n",
        "            log_q -= log_det\n",
        "        if not score_fn:\n",
        "            z_ = z\n",
        "            log_q = torch.zeros(len(z_), device=z_.device)\n",
        "\n",
        "            for param in self.parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "            for i in range(len(self.flows) - 1, -1, -1):\n",
        "                z_, log_det = self.flows[i].inverse(z_)\n",
        "                log_q += log_det\n",
        "            log_q += self.q0.log_prob(z_)\n",
        "\n",
        "            for param in self.parameters():\n",
        "              param.requires_grad = True\n",
        "\n",
        "        log_p = self.p.log_prob(z)\n",
        "        return torch.mean(log_q) - beta * torch.mean(log_p)\n",
        "\n",
        "    def sample(self, num_samples=1):\n",
        "        \"\"\"\n",
        "        Samples from flow-based approximate distribution\n",
        "        \"\"\"\n",
        "        z, log_q = self.q0(num_samples)\n",
        "        for flow in self.flows:\n",
        "            z, log_det = flow(z)\n",
        "            log_q -= log_det\n",
        "        return z, log_q\n",
        "\n",
        "    def log_prob(self, x):\n",
        "\n",
        "        log_q = torch.zeros(len(x), dtype=x.dtype, device=x.device)\n",
        "        z = x\n",
        "        for i in range(len(self.flows) - 1, -1, -1):\n",
        "            z, log_det = self.flows[i].inverse(z)\n",
        "            log_q += log_det\n",
        "        log_q += self.q0.log_prob(z)\n",
        "        return log_q\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "j5zToifulrpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstScaleLayer(nn.Module):\n",
        "    def __init__(self, scale=1.0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.scale_cpu = torch.tensor(scale)\n",
        "        self.register_buffer(\"scale\", self.scale_cpu)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * self.scale\n",
        "\n",
        "class ClampExp(nn.Module):\n",
        "    \"\"\"\n",
        "    Nonlinearity min(exp(lam * x), 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ClampExp, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        one = torch.tensor(1.0, device=x.device, dtype=x.dtype)\n",
        "        return torch.min(torch.exp(x), one)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers,\n",
        "        leaky=0.0,\n",
        "        score_scale=None,\n",
        "        output_fn=None,\n",
        "        output_scale=None,\n",
        "        init_zeros=False,\n",
        "        dropout=None,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        net = nn.ModuleList([])\n",
        "        for k in range(len(layers) - 2):\n",
        "            net.append(nn.Linear(layers[k], layers[k + 1]))\n",
        "            net.append(nn.LeakyReLU(leaky))\n",
        "        if dropout is not None:\n",
        "            net.append(nn.Dropout(p=dropout))\n",
        "        net.append(nn.Linear(layers[-2], layers[-1]))\n",
        "        if init_zeros:\n",
        "            nn.init.zeros_(net[-1].weight)\n",
        "            nn.init.zeros_(net[-1].bias)\n",
        "        if output_fn is not None:\n",
        "            if score_scale is not None:\n",
        "                net.append(ConstScaleLayer(score_scale))\n",
        "            if output_fn == \"sigmoid\":\n",
        "                net.append(nn.Sigmoid())\n",
        "            elif output_fn == \"relu\":\n",
        "                net.append(nn.ReLU())\n",
        "            elif output_fn == \"tanh\":\n",
        "                net.append(nn.Tanh())\n",
        "            elif output_fn == \"clampexp\":\n",
        "                net.append(ClampExp())\n",
        "            else:\n",
        "                NotImplementedError(\"This output function is not implemented.\")\n",
        "            if output_scale is not None:\n",
        "                net.append(ConstScaleLayer(output_scale))\n",
        "        self.net = nn.Sequential(*net)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "_VuF7JuOlECj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Set up RealNVP for 2D toy Experiments"
      ],
      "metadata": {
        "id": "0Jj-qxUB1qpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 2D Gaussian base distribution & target distribution\n",
        "\n",
        "base = DiagGaussian(2)\n",
        "target = CircularGaussianMixture()"
      ],
      "metadata": {
        "id": "GIE51pDpkcVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot target distribution\n",
        "grid_size = 200\n",
        "xx, yy = torch.meshgrid(torch.linspace(-3, 3, grid_size), torch.linspace(-3, 3, grid_size))\n",
        "zz = torch.cat([xx.unsqueeze(2), yy.unsqueeze(2)], 2).view(-1, 2)\n",
        "zz = zz.to(device)\n",
        "\n",
        "log_prob = target.log_prob(zz).to('cpu').view(*xx.shape)\n",
        "prob = torch.exp(log_prob)\n",
        "prob[torch.isnan(prob)] = 0\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\n",
        "plt.gca().set_aspect('equal', 'box')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nttdG8hElsxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define list of flows\n",
        "num_layers = 32\n",
        "flows = []\n",
        "for i in range(num_layers):\n",
        "    # Neural network with two hidden layers having 64 units each\n",
        "    # Last layer is initialized by zeros making training more stable\n",
        "    param_map = MLP([1, 64, 64, 2], init_zeros=True)\n",
        "    # Add flow layer\n",
        "    flows.append(AffineCouplingBlock(param_map))\n",
        "    # Swap dimensions\n",
        "    flows.append(Permute(2))\n",
        "\n",
        "# Construct flow model\n",
        "model = NormalizingFlow(base, flows)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "K2WRuQSBjdCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot initial flow distribution\n",
        "model.eval()\n",
        "log_prob = model.log_prob(zz).to('cpu').view(*xx.shape)\n",
        "prob = torch.exp(log_prob)\n",
        "prob[torch.isnan(prob)] = 0\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\n",
        "plt.gca().set_aspect('equal', 'box')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5GauMw7Clszo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train RealNVP"
      ],
      "metadata": {
        "id": "_4ms06p512eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iter = 4000\n",
        "num_samples = 2 ** 9\n",
        "show_iter = 500\n",
        "\n",
        "\n",
        "loss_hist = np.array([])\n",
        "\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "\n",
        "for it in tqdm(range(max_iter)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get training samples\n",
        "    x = target.sample(num_samples).to(device)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = model.forward_kld(x)\n",
        "\n",
        "    # Do backprop and optimizer step\n",
        "    if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Log loss\n",
        "    loss_hist = np.append(loss_hist, loss.to('cpu').data.numpy())\n",
        "\n",
        "    # Plot learned distribution\n",
        "    if (it + 1) % show_iter == 0:\n",
        "        model.eval()\n",
        "        log_prob = model.log_prob(zz)\n",
        "        model.train()\n",
        "        prob = torch.exp(log_prob.to('cpu').view(*xx.shape))\n",
        "        prob[torch.isnan(prob)] = 0\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\n",
        "        plt.gca().set_aspect('equal', 'box')\n",
        "        plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(loss_hist, label='loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AdAmWtWZpTCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot target distribution\n",
        "f, ax = plt.subplots(1, 2, sharey=True, figsize=(12, 5))\n",
        "\n",
        "log_prob = target.log_prob(zz).to('cpu').view(*xx.shape)\n",
        "prob = torch.exp(log_prob)\n",
        "prob[torch.isnan(prob)] = 0\n",
        "\n",
        "ax[0].pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\n",
        "\n",
        "ax[0].set_aspect('equal', 'box')\n",
        "ax[0].set_axis_off()\n",
        "ax[0].set_title('Target', fontsize=24)\n",
        "\n",
        "# Plot learned distribution\n",
        "model.eval()\n",
        "log_prob = model.log_prob(zz).to('cpu').view(*xx.shape)\n",
        "model.train()\n",
        "prob = torch.exp(log_prob)\n",
        "prob[torch.isnan(prob)] = 0\n",
        "\n",
        "ax[1].pcolormesh(xx, yy, prob.data.numpy(), cmap='coolwarm')\n",
        "\n",
        "ax[1].set_aspect('equal', 'box')\n",
        "ax[1].set_axis_off()\n",
        "ax[1].set_title('Real NVP', fontsize=24)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QWSf5B0rpTEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Training for MNIST datasets - 5.2 Quiz"
      ],
      "metadata": {
        "id": "eGURvVOf2mAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-1. Prepares MNIST datasets"
      ],
      "metadata": {
        "id": "2se6HKmH4RWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare training data\n",
        "batch_size = 128\n",
        "\n",
        "transform = tv.transforms.Compose([tv.transforms.ToTensor(),\n",
        "                                   tv.transforms.Normalize((0.5), (1.0))])\n",
        "\n",
        "class_label = [1,2,3] # Or Set it as you want [1], [4], [1, 9], ... digit 0~9\n",
        "train_dataset = tv.datasets.MNIST('datasets/', train=True, download=True, transform=transform)\n",
        "class_one_indices = [i for i, (_, label) in enumerate(train_dataset) if label in class_label]\n",
        "class_one_dataset_train = torch.utils.data.Subset(train_dataset, class_one_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(class_one_dataset_train,  batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "test_dataset = tv.datasets.MNIST('datasets/', train=False, download=True, transform=transform)\n",
        "class_one_indices = [i for i, (_, label) in enumerate(test_dataset) if label == class_label]\n",
        "class_one_dataset_test = torch.utils.data.Subset(test_dataset, class_one_indices)\n",
        "test_loader = torch.utils.data.DataLoader(class_one_dataset_test, batch_size=batch_size)\n",
        "\n",
        "train_iter = iter(train_loader)"
      ],
      "metadata": {
        "id": "dwm5cmSbpTGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-2. Define your own NF model - Quiz\n",
        "* Write down answer1~4 and Train you model on MNIST datasets"
      ],
      "metadata": {
        "id": "DTLlxrdm4BY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Gaussian base distribution\n",
        "# answer1 = ?\n",
        "# answer2 = ?\n",
        "# answer3 = ?\n",
        "# answer4 = ?\n",
        "\n",
        "mnist_base = DiagGaussian(answer1)\n",
        "\n",
        "# Define list of flows\n",
        "num_layers = 32 # Or Set it as you want (integer) 8, 12, 16, 32, ...\n",
        "flows = []\n",
        "for i in range(num_layers):\n",
        "    param_map = MLP([answer2, 64, 64, answer3], init_zeros=True)\n",
        "    # Add flow layer\n",
        "    flows.append(AffineCouplingBlock(param_map))\n",
        "    # Swap dimensions\n",
        "    flows.append(Permute(answer4))\n",
        "\n",
        "# Construct flow model\n",
        "my_model = NormalizingFlow(mnist_base, flows)\n",
        "my_model = my_model.to(device)"
      ],
      "metadata": {
        "id": "h7zwnaUB4A6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "max_iter = 10000\n",
        "\n",
        "my_model.train()\n",
        "optimizer = torch.optim.Adam(my_model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "\n",
        "loss_hist = np.array([])\n",
        "\n",
        "for i in tqdm(range(max_iter)):\n",
        "    try:\n",
        "        x, y = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        x, y = next(train_iter)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = my_model.forward_kld(x.reshape(batch_size, -1).to(device))\n",
        "\n",
        "    if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_hist = np.append(loss_hist, loss.detach().to('cpu').numpy())"
      ],
      "metadata": {
        "id": "M1azv7aR2rDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(loss_hist, label='loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W_P3cuFaJABg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model samples\n",
        "num_sample = 15\n",
        "z_sample, _ = my_model.sample(num_sample)\n",
        "z_sample = z_sample.reshape(num_sample, 28, 28).unsqueeze(1)\n",
        "z_sample = torch.clamp(z_sample, 0., 1.)\n",
        "plt.figure(figsize=(5, 15))\n",
        "plt.imshow(np.transpose(tv.utils.make_grid(z_sample, nrow=3).cpu().numpy(), (1, 2, 0)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HoqsRRm5xw9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dfyndeS0kXC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}